{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import feather\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "\n",
    "sys.path.append(\"/Users/ueda/Desktop/kaggle/mynavi/code/src/\")\n",
    "from logger import setup_logger, LOGGER\n",
    "from trainer import train_lgbm\n",
    "from util_tool import reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already directory created\n"
     ]
    }
   ],
   "source": [
    "# ===============\n",
    "# Constants\n",
    "# ===============\n",
    "DATA_DIR = \"../input/\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "SUB_PATH = os.path.join(DATA_DIR, \"sample_submit.csv\")\n",
    "#FOLDS_PATH = \"../input/mynavi_Stratifiedfold01.feather\"   #289381102       #folds気をつけて\n",
    "#FOLDS_PATH = \"../input/mynavi_Stratifiedfold_access.feather\"  #有望\n",
    "\n",
    "FOLDS_PATH = \"../input/mynavi_Stratifiedfold_address.feather\" #274592066 最強\n",
    "#FOLDS_PATH = \"../input/mynavi_Stratifiedfold_rent.feather\"     #299584731\n",
    "#FOLDS_PATH = \"../input/mynavi_Stratifiedfold_same_build.feather\" # 293669458\n",
    "LOGGER_PATH = \"log.txt\"\n",
    "ID_COLUMN = \"id\"\n",
    "\n",
    "\n",
    "EXP_ID = \"exp0_+address_poisson_outlin+meta\"\n",
    "\n",
    "SAVE_PATH = f'../output/{EXP_ID}'\n",
    "try:\n",
    "    os.mkdir(SAVE_PATH)\n",
    "except:\n",
    "    print(\"Already directory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 20:30:45,798 - INFO - logger set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============\n",
    "# Settings\n",
    "# ===============\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "SEED = 0\n",
    "N_SPLITS = 5\n",
    "LOG = False\n",
    "LGBM_PARAMS = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',   #rf, gbdt\n",
    "    'objective': 'poisson',    #regression, gamma, poisson gamma早すぎ\n",
    "    'metric': \"rmse\",\n",
    "    'learning_rate': 0.2,\n",
    "    'max_depth': 8,            #9\n",
    "    'num_leaves': int(.7 * 8 ** 2),        #15 < int()\n",
    "    'subsample': 0.9,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.8, # 0.4, 0.7, 0.8で271219355 0.8良さげ\n",
    "    'max_bin': 255,\n",
    "    'reg_alpha': 0.1,        # 0.1\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_samples': 20,   #20 vs 10\n",
    "    'min_gain_to_split': 0.02,\n",
    "    'min_data_in_bin': 3,\n",
    "    'bin_construct_sample_cnt': 5000,\n",
    "    'cat_l2': 5,    #5でも10でも\n",
    "    'verbose': -1,\n",
    "    'nthread': -1,\n",
    "    'seed': SEED,\n",
    "}\n",
    "LGBM_FIT_PARAMS = {\n",
    "    'num_boost_round': 50000,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'verbose_eval': 5000,\n",
    "}\n",
    "\n",
    "use_cols = []\n",
    "\n",
    "drop_columns = ['fold_id', 'id', 'キッチン', 'バス・トイレ', '周辺環境',\n",
    "                '室内設備', '所在階', '放送・通信', '間取り',\n",
    "                '築年数', '賃料', '面積', '駐車場', \"路線_0\", \"駅_0\", \"方角\", #\"所在地\", #\"same_build\",\n",
    "                 \"路線_1\", \"駅_1\", \"路線_2\", \"駅_2\", \"契約期間\", '建物構造', 'アクセス', #\"地域_n丁目\", \"地域名\",# \"catuse_駅_0\", \"catuse_路線_0\"\n",
    "]\n",
    "\n",
    "#ategorical_features = [\"catuse_駅_0\", \"catuse_路線_0\", #\"地域名\",\n",
    "#                        \"cat_int間取り\", \"cat_building_height\",'所在_区'\n",
    "#]\n",
    "\n",
    "categorical_features = [\"catuse_駅_0\", \"catuse_路線_0\", \"所在地\", \"地域名\", \"地域_n丁目\",\n",
    "                        \"cat_int間取り\", \"cat_building_height\",'所在_区',\n",
    "] #258442952\n",
    "\n",
    "#TE_columns = [\"所在_区\", \"間取りtype\", 'catuse_路線_0', 'catuse_駅_0',\n",
    "#           \"cat_int間取り\", \"cat_building_height\", \"地域名\", '地域_n丁目', \"same_build\"\n",
    "#]\n",
    "#drop_columns = drop_columns + [col for col in categorical_features if col in TE_columns]\n",
    "#categorical_features = [cat_col for cat_col in categorical_features if cat_col not in TE_columns]\n",
    "\n",
    "FEATURES = [\n",
    "    \"../code/feature_csv/base_feature1.feather\",\n",
    "    \"../code/feature_csv/nonleak_group_feature2.feather\",\n",
    "    #'../code/feature_csv/nonleak_nearStation_group_feature.feather',\n",
    "    #'../code/feature_csv/near_access_feature.feather',\n",
    "    \"../code/feature_csv/word_contain_sparse_feature.feather\",\n",
    "    \"../code/feature_csv/access_feature.feather\",\n",
    "    \"../code/feature_csv/address_latlng.feather\",\n",
    "    #\"../code/feature_csv/meta_features.feather\"\n",
    "]\n",
    "setup_logger(out_file=LOGGER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "def load_feature_feather(df, filename):\n",
    "    feature = feather.read_dataframe(filename)\n",
    "    df = pd.concat((df, feature), axis=1)\n",
    "    del feature\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def load_feature_csv(df, filename):\n",
    "    feature = pd.csv(filename)\n",
    "    df = pd.concat((df, feature), axis=1)\n",
    "    del feature\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def modify_rent(target, df_input):\n",
    "    \"\"\" id == 5776 で賃料の0が一つ多い\"\"\"\n",
    "    target = np.where(df_input[\"id\"]==5776, 120350, target)\n",
    "    \n",
    "    return target\n",
    "\n",
    "\n",
    "\"\"\"def TE_feature(df_main, df_add, TE_columns = TE_columns):\n",
    "    df_out = pd.DataFrame()\n",
    "    add_TE_cols = [f\"TE_{col}_mul_面積\" for col in TE_columns]\n",
    "    tmp_cols = [f\"TE_{col}\" for col in TE_columns]\n",
    "    TE_df = df_add[TE_columns]\n",
    "    TE_df.columns = tmp_cols\n",
    "    \n",
    "    TE_mul_df = pd.DataFrame(TE_df[tmp_cols].values * df_main[\"float面積\"].values.reshape(-1,1), columns=add_TE_cols)\n",
    "    TE_mul_df.index = df_main.index\n",
    "    TE_df.index = df_main.index\n",
    "    \n",
    "    TE_mul_df[\"std_TE賃料\"] = TE_mul_df.std(axis=1)\n",
    "    TE_mul_df[\"min_TE賃料\"] = TE_mul_df.min(axis=1)\n",
    "    TE_mul_df[\"max_TE賃料\"] = TE_mul_df.max(axis=1)\n",
    "    TE_mul_df[\"mean_TE賃料\"] = TE_mul_df.mean(axis=1)\n",
    "    \n",
    "    df_out = pd.concat([TE_df, TE_mul_df], axis=1)\n",
    "    \n",
    "    return df_out\"\"\"\n",
    "\n",
    "def calc_loss(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 20:30:47,146 - INFO - [load data] done in 1 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"load data\"):\n",
    "    train = feather.read_dataframe(FOLDS_PATH)\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    if LOG:\n",
    "        y = np.log1p(train['賃料'].copy())\n",
    "    else:\n",
    "        y = train['賃料'].copy()\n",
    "    folds = train[[\"fold_id\"]].copy()\n",
    "    n_train = len(train)\n",
    "    train = train.append(test).reset_index(drop=True)\n",
    "    del test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 20:30:51,114 - INFO - [load featuers] done in 4 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"load featuers\"):\n",
    "    for f in FEATURES:\n",
    "        train = load_feature_feather(train, f)\n",
    "        \n",
    "    #train[\"same_build\"] = train[\"building_floor\"].astype(str) + train[\"地域_n丁目\"] + train[\"int築年\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifify Done\n",
      "****************\n",
      "DROP DONE!\n",
      "****************\n",
      "Memory usage of dataframe is 230.25 MB\n",
      "column =  959\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "Memory usage after optimization is: 96.01 MB\n",
      "Decreased by 58.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 20:31:37,348 - INFO - [preprocessing] done in 46 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"preprocessing\"):\n",
    "    for c in categorical_features:\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].astype(\"str\").values))\n",
    "        train[c] = lbl.transform(list(train[c].astype(\"str\").values))\n",
    "\n",
    "    \n",
    "    \"\"\"記入ミスの修正\"\"\"\n",
    "    if \"id\" in train.columns:\n",
    "        y = modify_rent(y, train[:n_train])\n",
    "        print(\"Modifify Done\")\n",
    "        print(\"****************\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        train.drop(drop_columns, axis=1, inplace=True)\n",
    "        print(\"DROP DONE!\")\n",
    "        print(\"****************\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \"\"\"importtanceで区切るなら\"\"\"\n",
    "    if len(use_cols)==0:\n",
    "        test = train[n_train:]\n",
    "        train = train[:n_train]\n",
    "    \n",
    "    elif len(use_cols)>0:\n",
    "        test = test[use_cols]\n",
    "        train = train[use_cols]\n",
    "        \n",
    "    try:\n",
    "        train.drop(drop_columns, axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \"\"\"外れ値分類の特量追加\"\"\"\n",
    "    out = np.percentile(y, [85])\n",
    "    outlin =  np.where(y > out,1,0)\n",
    "    model = lgbm.LGBMClassifier().fit(train, outlin)\n",
    "    train[\"out\"] = outlin\n",
    "    test[\"out\"] = model.predict(test)\n",
    "    \n",
    "    \n",
    "    train = reduce_mem_usage(train)\n",
    "    train.to_feather(f\"../features/{EXP_ID}_df.feather\")\n",
    "        \n",
    "    #gc.collect()\n",
    "    #target_test = pd.read_csv(f'../code/feature_csv/target_encoding_test.csv')\n",
    "    #test = pd.concat([test, TE_feature(test, target_test)], axis=1)\n",
    "    \n",
    "    features = list(test.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train.shape\\nold_df = feather.read_dataframe(\"../features/exp0_+address_poisson_outlin_df.feather\")\\n[col for col in test.columns if col not in old_df.columns]\\n#[col for col in old_df.columns if col not in test.columns]\\nymp = [col for col in test.columns if col in old_df.columns]\\n\\nfor col in ymp:\\n    if (train[col]==old_df[col]).nunique() == 2:\\n        if (np.isnan(train[col]).values == (train[col] != old_df[col]).values).sum() != train.shape[0]:\\n            print((train[col] == old_df[col]).value_counts())'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train.shape\n",
    "old_df = feather.read_dataframe(\"../features/exp0_+address_poisson_outlin_df.feather\")\n",
    "[col for col in test.columns if col not in old_df.columns]\n",
    "#[col for col in old_df.columns if col not in test.columns]\n",
    "ymp = [col for col in test.columns if col in old_df.columns]\n",
    "\n",
    "for col in ymp:\n",
    "    if (train[col]==old_df[col]).nunique() == 2:\n",
    "        if (np.isnan(train[col]).values == (train[col] != old_df[col]).values).sum() != train.shape[0]:\n",
    "            print((train[col] == old_df[col]).value_counts())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[290]\tvalid's rmse: 16347.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 20:31:50,916 - INFO - Best Iteration: 290\n",
      "2019-11-06 20:31:53,197 - INFO - [fold 0] done in 16 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1009]\tvalid's rmse: 14764.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 20:32:50,431 - INFO - Best Iteration: 1009\n",
      "2019-11-06 20:32:53,855 - INFO - [fold 1] done in 61 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n"
     ]
    }
   ],
   "source": [
    "with timer(\"train\"):\n",
    "    y_oof = np.empty(len(train), )\n",
    "    y_test = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "\n",
    "    for fold_id in range(N_SPLITS):\n",
    "        with timer(f\"fold {fold_id}\"):\n",
    "            x_train, y_train = train[folds.fold_id != fold_id], y[folds.fold_id != fold_id]\n",
    "            x_val, y_val = train[folds.fold_id == fold_id], y[folds.fold_id == fold_id]\n",
    "            \n",
    "            #TE_fold = pd.read_csv(f'../code/feature_csv/target_encoding_groupfold{fold_id}.csv')\n",
    "            #x_train = pd.concat([x_train, TE_feature(x_train, TE_fold[TE_fold[\"fold_id\"]!=fold_id])], axis=1)\n",
    "            #x_val = pd.concat([x_val, TE_feature(x_val, TE_fold[TE_fold[\"fold_id\"]==fold_id])], axis=1)\n",
    "            \n",
    "            y_pred_valid, y_pred_test, train_loss, valid_loss, importances, best_iter = train_lgbm(\n",
    "                x_train, y_train, x_val, y_val, test,\n",
    "                categorical_features=categorical_features,\n",
    "                feature_name=features,\n",
    "                fold_id=fold_id,\n",
    "                lgb_params=LGBM_PARAMS,\n",
    "                fit_params=LGBM_FIT_PARAMS,\n",
    "                model_name=EXP_ID,\n",
    "                loss_func=calc_loss,\n",
    "                rank=False,\n",
    "                calc_importances=True\n",
    "            )\n",
    "            y_oof[folds.fold_id == fold_id] = y_pred_valid\n",
    "            y_test.append(y_pred_test)\n",
    "            feature_importances = pd.concat([feature_importances, importances], axis=0, sort=False)\n",
    "\n",
    "    feature_importances.to_csv(os.path.join(SAVE_PATH, \"feature_importances.csv\"), index=False)\n",
    "    if LOG:\n",
    "        y = np.expm1(y)\n",
    "        y_oof = np.expm1(y_oof)\n",
    "        y_test = np.expm1(y_test)\n",
    "    score = calc_loss(y, y_oof)\n",
    "    y_test = np.mean(y_test, axis=0)\n",
    "    yoof_and_target = pd.DataFrame()\n",
    "    yoof_and_target[\"賃料\"] = y\n",
    "    yoof_and_target[f'{EXP_ID}'] = y_oof\n",
    "    yoof_and_target.to_csv(os.path.join(SAVE_PATH, \"yoof_and_target.csv\"), index=False)\n",
    "    yoof_and_target.to_csv(f'../output/vs_trains/{EXP_ID}.csv', index=False)\n",
    "    \n",
    "    test_result = pd.DataFrame()\n",
    "    test_result[f\"{EXP_ID}\"] = y_test\n",
    "    test_result.to_csv(f'../output/test_results/{EXP_ID}.csv', index=False)\n",
    "    \n",
    "    LOGGER.info(f'CV={score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"if fold_id does not drop \"\"\"\n",
    "#(x_train[x_train[\"fold_id\"]!=0][\"fold_id\"] == TE_feature[TE_feature[\"fold_id\"]!=0][\"fold_id\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TE_feature[TE_columns][\"所在_区\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"sub\"):\n",
    "    sub = pd.read_csv(SUB_PATH, header=None)\n",
    "    sub[1] = y_test\n",
    "    LOGGER.info(f'len sub={len(sub)}')\n",
    "    sub.to_csv(os.path.join(SAVE_PATH,f'{EXP_ID}.csv'), index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"importtances visualization\"\"\"\n",
    "cols = importances[[\"feature\", \"gain\"]].groupby(\"feature\").mean().sort_values(\n",
    "    by=\"gain\", ascending=False)[:1000].index\n",
    "\n",
    "best_features = importances.loc[importances.feature.isin(cols)]\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.barplot(x=\"gain\", y=\"feature\", data=best_features.sort_values(by=\"gain\", ascending=False))\n",
    "plt.title(f'LightGBM Features (avg over folds)_{EXP_ID}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_PATH,(f'lgbm_importances_{EXP_ID}.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features\n",
    "#TE_feature[TE_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "importances.sort_values(\"gain\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = list(importances[importances[\"gain\"]>30000][\"feature\"].values)\n",
    "print(len(use_cols), len(train.columns))\n",
    "use_cols = [col for col in use_cols if col in train.columns]\n",
    "use_cols = use_cols + [col for col in categorical_features if col not in use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoof_and_target[yoof_and_target[\"賃料\"] == yoof_and_target[\"賃料\"].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[x_train[\"max_TE賃料\"]==x_train[\"max_TE賃料\"].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.reset_index()\n",
    "[test2[\"out\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_model = pd.read_csv(\"../output/exp1_poisson_upper100000/exp1_poisson_upper100000_100000.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = sub[[0]]\n",
    "ens[\"1\"] = (np.where([test2[\"out\"]==1], up_model[1], sub[1])).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.to_csv('ens.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
