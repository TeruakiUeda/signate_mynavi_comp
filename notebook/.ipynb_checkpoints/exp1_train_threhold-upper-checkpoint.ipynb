{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import feather\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "\n",
    "sys.path.append(\"/Users/ueda/Desktop/kaggle/mynavi/code/src/\")\n",
    "from logger import setup_logger, LOGGER\n",
    "from trainer import train_lgbm\n",
    "from util_tool import reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already directory created\n"
     ]
    }
   ],
   "source": [
    "# ===============\n",
    "# Constants\n",
    "# ===============\n",
    "DATA_DIR = \"../input/\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.csv\")\n",
    "SUB_PATH = os.path.join(DATA_DIR, \"sample_submit.csv\")\n",
    "FOLDS_PATH = \"../input/mynavi_Stratifiedfold_address_under300000.feather\"  #folds気をつけて\n",
    "LOGGER_PATH = \"log.txt\"\n",
    "ID_COLUMN = \"id\"\n",
    "\n",
    "USE_TRAIN_THREHOLD = True\n",
    "THREHOLD = 300000\n",
    "\n",
    "EXP_ID = f\"exp1_poisson_under{THREHOLD}_new\"\n",
    "SAVE_PATH = f'../output/{EXP_ID}'\n",
    "\n",
    "try:\n",
    "    os.mkdir(SAVE_PATH)\n",
    "except:\n",
    "    print(\"Already directory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 21:15:45,207 - INFO - logger set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============\n",
    "# Settings\n",
    "# ===============\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "SEED = 0\n",
    "N_SPLITS = 5\n",
    "LOG = False\n",
    "\n",
    "LGBM_PARAMS = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'poisson',    #regression, gamma, poisson gamma早すぎ\n",
    "    'metric': \"rmse\",\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 9,\n",
    "    'num_leaves': int(0.7*9**2),\n",
    "    'subsample': 0.9,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'max_bin': 255,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_samples': 5,\n",
    "    'min_gain_to_split': 0.02,\n",
    "    'min_data_in_bin': 3,\n",
    "    'bin_construct_sample_cnt': 5000,\n",
    "    'cat_l2': 5,\n",
    "    'verbose': -1,\n",
    "    'nthread': -1,\n",
    "    'seed': SEED,\n",
    "}\n",
    "LGBM_FIT_PARAMS = {\n",
    "    'num_boost_round': 50000,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'verbose_eval': 5000,\n",
    "}\n",
    "\n",
    "use_cols = []\n",
    "\n",
    "drop_columns = ['fold_id', 'id', 'キッチン', 'バス・トイレ', '周辺環境',\n",
    "                '室内設備', '所在階', '放送・通信', '間取り',\n",
    "                '築年数', '賃料', '面積', '駐車場', \"路線_0\", \"駅_0\", \"方角\",\n",
    "                 \"路線_1\", \"駅_1\", \"路線_2\", \"駅_2\", \"契約期間\", '建物構造', 'アクセス', #\"same_build\"\n",
    "]\n",
    "\n",
    "categorical_features = ['所在_区', \"catuse_駅_0\", \"catuse_路線_0\", \"所在地\", \"地域名\", \"地域_n丁目\",\n",
    "                        \"cat_int間取り\", \"cat_building_height\", \"same_build\"\n",
    "]\n",
    "\n",
    "\n",
    "TE_columns = [#\"所在_区\", #'catuse_路線_0', 'catuse_駅_0', \n",
    "              #\"所在_区\", #\"間取りtype\", #'catuse_路線_0', 'catuse_駅_0', \"int築年\"\n",
    "              #\"cat_int間取り\", #\"cat_building_height\" # \"定期借家\", '一戸建て'\n",
    "]\n",
    "\n",
    "FEATURES = [\n",
    "    \"../code/feature_csv/base_feature1.feather\",\n",
    "    \"../code/feature_csv/nonleak_group_feature2.feather\",\n",
    "    #'../code/feature_csv/nonleak_nearStation_group_feature.feather',\n",
    "    #'../code/feature_csv/near_access_feature.feather',\n",
    "    \"../code/feature_csv/word_contain_sparse_feature.feather\",\n",
    "    \"../code/feature_csv/access_feature.feather\",\n",
    "    \"../code/feature_csv/address_latlng.feather\",\n",
    "    \"../code/feature_csv/meta_features.feather\"\n",
    "]\n",
    "setup_logger(out_file=LOGGER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "def load_feature_feather(df, filename, use_index):\n",
    "    feature = feather.read_dataframe(filename)\n",
    "    feature = feature.loc[use_index]\n",
    "    df = pd.concat((df, feature), axis=1)\n",
    "    del feature\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def load_feature_csv(df, filename, use_index):\n",
    "    feature = pd.csv(filename)\n",
    "    feature = feature.loc[use_index]\n",
    "    df = pd.concat((df, feature), axis=1)\n",
    "    del feature\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def modify_rent(target, df_input):\n",
    "    \"\"\" id == 5776 で賃料の0が一つ多い\"\"\"\n",
    "    target = np.where(df_input[\"id\"]==5776, 120350, target)\n",
    "    \n",
    "    return target\n",
    "\n",
    "\n",
    "def calc_loss(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 21:15:46,672 - INFO - [load data] done in 1 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"load data\"):\n",
    "    train = feather.read_dataframe(FOLDS_PATH)\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    folds = train[[\"fold_id\"]].copy()\n",
    "    \n",
    "    if LOG:\n",
    "        y = np.log1p(train['賃料'].copy())\n",
    "    else:\n",
    "        y = train['賃料'].copy()\n",
    "    \n",
    "    if USE_TRAIN_THREHOLD:\n",
    "        folds = folds[y < THREHOLD]\n",
    "        folds = folds.reset_index(drop=True)\n",
    "        y = y[y < THREHOLD]\n",
    "        y = y.reset_index(drop=True)\n",
    "    \n",
    "    n_train = len(train)\n",
    "    train = train.append(test).reset_index(drop=True)\n",
    "    del test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_index = train[\"id\"]-1\n",
    "use_index.name = None\n",
    "\n",
    "n_train = len(y)\n",
    "train = train.set_index(use_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 21:15:52,720 - INFO - [load featuers] done in 6 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"load featuers\"):\n",
    "    for f in FEATURES:\n",
    "        train = load_feature_feather(train, f, use_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 232.02 MB\n",
      "column =  1013\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "Memory usage after optimization is: 100.42 MB\n",
      "Decreased by 56.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-06 21:16:38,051 - INFO - [preprocessing] done in 45 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"preprocessing\"):\n",
    "    for c in categorical_features:\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].astype(\"str\").values))\n",
    "        train[c] = lbl.transform(list(train[c].astype(\"str\").values))\n",
    "        \n",
    "    \n",
    "    \"\"\"記入ミスの修正\"\"\"\n",
    "    try:\n",
    "        y = modify_rent(y, train[:n_train])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        train.drop(drop_columns, axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \"\"\"importtanceで区切るなら\"\"\"\n",
    "    if len(use_cols)==0:\n",
    "        test = train[n_train:]\n",
    "        train = train[:n_train]\n",
    "    \n",
    "    elif len(use_cols)>0:\n",
    "        test = test[use_cols]\n",
    "        train = train[use_cols]\n",
    "    \n",
    "    \n",
    "    \"\"\"外れ値分類の特量追加\"\"\"\n",
    "    out = np.percentile(y, [90])\n",
    "    outlin =  np.where(y > out,1,0)\n",
    "    model = lgbm.LGBMClassifier().fit(train, outlin)\n",
    "    train[\"out\"] = outlin\n",
    "    test[\"out\"] = model.predict(test)\n",
    "    \n",
    "    \n",
    "    train = reduce_mem_usage(train)\n",
    "    train = train.reset_index(drop=True)\n",
    "    train.to_feather(f\"../features/{EXP_ID}_df.feather\")\n",
    "    \n",
    "        \n",
    "    features = list(train.columns.values)\n",
    "    \n",
    "    #gc.collect()\n",
    "    \n",
    "    categorical_features = [cat_col for cat_col in categorical_features if cat_col not in TE_columns]\n",
    "    \n",
    "    target_test = pd.read_csv(f'../code/feature_csv/target_encoding_test.csv')\n",
    "    test[TE_columns] = target_test[TE_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n"
     ]
    }
   ],
   "source": [
    "with timer(\"train\"):\n",
    "    y_oof = np.empty(len(train), )\n",
    "    y_test = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "\n",
    "    for fold_id in range(N_SPLITS):\n",
    "        with timer(f\"fold {fold_id}\"):\n",
    "            x_train, y_train = train[folds.fold_id != fold_id], y[folds.fold_id != fold_id]\n",
    "            x_val, y_val = train[folds.fold_id == fold_id], y[folds.fold_id == fold_id]\n",
    "            \n",
    "            #TE_feature = pd.read_csv(f'../code/feature_csv/target_encoding_groupfold{fold_id}.csv')\n",
    "            #x_train[TE_columns] = TE_feature[TE_feature[\"fold_id\"]!=fold_id][TE_columns]\n",
    "            #x_val[TE_columns] = TE_feature[TE_feature[\"fold_id\"]==fold_id][TE_columns]\n",
    "\n",
    "            y_pred_valid, y_pred_test, train_loss, valid_loss, importances, best_iter = train_lgbm(\n",
    "                x_train, y_train, x_val, y_val, test,\n",
    "                categorical_features=categorical_features,\n",
    "                feature_name=features,\n",
    "                fold_id=fold_id,\n",
    "                lgb_params=LGBM_PARAMS,\n",
    "                fit_params=LGBM_FIT_PARAMS,\n",
    "                model_name=EXP_ID,\n",
    "                loss_func=calc_loss,\n",
    "                rank=False,\n",
    "                calc_importances=True\n",
    "            )\n",
    "            y_oof[folds.fold_id == fold_id] = y_pred_valid\n",
    "            y_test.append(y_pred_test)\n",
    "            feature_importances = pd.concat([feature_importances, importances], axis=0, sort=False)\n",
    "\n",
    "    feature_importances.to_csv(os.path.join(SAVE_PATH, \"feature_importances.csv\"), index=False)\n",
    "    if LOG:\n",
    "        y = np.expm1(y)\n",
    "        y_oof = np.expm1(y_oof)\n",
    "        y_test = np.expm1(y_test)\n",
    "    score = calc_loss(y, y_oof)\n",
    "    y_test = np.mean(y_test, axis=0)\n",
    "    #np.save(os.path.join(SAVE_PATH, \"oof.npy\"), y_oof)\n",
    "    #np.save(os.path.join(SAVE_PATH, \"y.npy\"), y)\n",
    "    yoof_and_target = pd.DataFrame()\n",
    "    yoof_and_target[\"賃料\"] = y\n",
    "    yoof_and_target[f'{EXP_ID}'] = y_oof\n",
    "    yoof_and_target.to_csv(os.path.join(SAVE_PATH, \"yoof_and_target.csv\"), index=False)\n",
    "    yoof_and_target.to_csv(f'../output/vs_trains/{EXP_ID}.csv', index=False)\n",
    "    \n",
    "    test_result = pd.DataFrame()\n",
    "    test_result[f\"{EXP_ID}\"] = y_test\n",
    "    test_result.to_csv(f'../output/test_results/{EXP_ID}.csv', index=False)\n",
    "    \n",
    "    LOGGER.info(f'CV={score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_child_sample  CV=395513950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"if fold_id does not drop \"\"\"\n",
    "#(x_train[x_train[\"fold_id\"]!=0][\"fold_id\"] == TE_feature[TE_feature[\"fold_id\"]!=0][\"fold_id\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TE_feature[TE_columns][\"所在_区\"].value_counts()\n",
    "THREHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"sub\"):\n",
    "    sub = pd.read_csv(SUB_PATH, header=None)\n",
    "    sub[1] = y_test\n",
    "    LOGGER.info(f'len sub={len(sub)}')\n",
    "    sub.to_csv(os.path.join(SAVE_PATH,f'{EXP_ID}_{THREHOLD}.csv'), index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"importtances visualization\"\"\"\n",
    "cols = importances[[\"feature\", \"gain\"]].groupby(\"feature\").mean().sort_values(\n",
    "    by=\"gain\", ascending=False)[:1000].index\n",
    "\n",
    "best_features = importances.loc[importances.feature.isin(cols)]\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.barplot(x=\"gain\", y=\"feature\", data=best_features.sort_values(by=\"gain\", ascending=False))\n",
    "plt.title(f'LightGBM Features (avg over folds)_{EXP_ID}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_PATH,(f'lgbm_importances_{EXP_ID}.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features\n",
    "#TE_feature[TE_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "importances.sort_values(\"gain\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = list(importances[importances[\"gain\"]>10000][\"feature\"].values)\n",
    "len(use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoof_and_target[yoof_and_target[\"賃料\"] == yoof_and_target[\"賃料\"].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
