{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import feather\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "import optuna\n",
    "\n",
    "sys.path.append(\"/Users/ueda/Desktop/kaggle/mynavi/code/src/\")\n",
    "from logger import setup_logger, LOGGER\n",
    "from trainer import train_lgbm\n",
    "from util_tool import reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already directory created\n"
     ]
    }
   ],
   "source": [
    "# ===============\n",
    "# Constants\n",
    "# ===============\n",
    "DATA_DIR = \"../input/\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.csv\")\n",
    "#META_PATH = os.path.join(DATA_DIR, \"meta.csv\")\n",
    "#META_PATH2 = os.path.join(DATA_DIR, \"minincome_clean.csv\")\n",
    "SUB_PATH = os.path.join(DATA_DIR, \"sample_submit.csv\")\n",
    "FOLDS_PATH = \"../input/mynavi_Stratifiedfold01.feather\"  #folds気をつけて\n",
    "LOGGER_PATH = \"log.txt\"\n",
    "ID_COLUMN = \"id\"\n",
    "\n",
    "\n",
    "EXP_ID = \"exp0_morimori_feature_reduce\"\n",
    "\n",
    "SAVE_PATH = f'../output/{EXP_ID}'\n",
    "try:\n",
    "    os.mkdir(SAVE_PATH)\n",
    "except:\n",
    "    print(\"Already directory created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-04 15:35:24,706 - INFO - logger set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============\n",
    "# Settings\n",
    "# ===============\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "SEED = 0\n",
    "N_SPLITS = 5\n",
    "LOG = False\n",
    "LGBM_PARAMS = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'poisson',    #regression, gamma, poisson gamma早すぎ\n",
    "    'metric': \"rmse\",\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 15,\n",
    "    'subsample': 0.9,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'max_depth': 9,\n",
    "    'max_bin': 255,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'min_child_samples': 20,\n",
    "    'min_gain_to_split': 0.02,\n",
    "    'min_data_in_bin': 3,\n",
    "    'bin_construct_sample_cnt': 5000,\n",
    "    'cat_l2': 5,\n",
    "    'verbose': -1,\n",
    "    'nthread': -1,\n",
    "    'seed': SEED,\n",
    "}\n",
    "LGBM_FIT_PARAMS = {\n",
    "    'num_boost_round': 50000,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'verbose_eval': 5000,\n",
    "}\n",
    "\n",
    "use_cols = []\n",
    "\n",
    "drop_columns = ['fold_id', 'id', 'キッチン', 'バス・トイレ', '周辺環境',\n",
    "                '室内設備', '所在地', '所在階', '放送・通信', '間取り',\n",
    "                '築年数', '賃料', '面積', '駐車場', \"路線_0\", \"駅_0\", \"方角\",\n",
    "                 \"路線_1\", \"駅_1\", \"路線_2\", \"駅_2\", \"契約期間\", '建物構造','アクセス', \"地域名\", \"地域_n丁目\"\n",
    "]\n",
    "\n",
    "categorical_features = ['所在_区', \"catuse_駅_0\", \"catuse_路線_0\",\n",
    "                        \"cat_int間取り\", \"cat_building_height\" #, '建物構造'\n",
    "]\n",
    "\n",
    "\n",
    "TE_columns = [#\"所在_区\", #'catuse_路線_0', 'catuse_駅_0', \n",
    "              #\"所在_区\", #\"間取りtype\", #'catuse_路線_0', 'catuse_駅_0', \"int築年\"\n",
    "              #\"cat_int間取り\", #\"cat_building_height\" # \"定期借家\", '一戸建て'\n",
    "]\n",
    "\n",
    "FEATURES = [\n",
    "    \"../code/feature_csv/base_feature1.feather\",\n",
    "    #\"../code/feature_csv/base_group_feature2.feather\",\n",
    "    \"../code/feature_csv/nonleak_group_feature2.feather\",\n",
    "    #'../code/feature_csv/nonleak_nearStation_group_feature.feather',\n",
    "    #'../code/feature_csv/near_access_feature.feather',\n",
    "    \"../code/feature_csv/word_contain_sparse_feature.feather\",\n",
    "    \"../code/feature_csv/access_feature.feather\"\n",
    "]\n",
    "setup_logger(out_file=LOGGER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "def load_feature_feather(df, filename):\n",
    "    feature = feather.read_dataframe(filename)\n",
    "    df = pd.concat((df, feature), axis=1)\n",
    "    del feature\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def load_feature_csv(df, filename):\n",
    "    feature = pd.csv(filename)\n",
    "    df = pd.concat((df, feature), axis=1)\n",
    "    del feature\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def calc_loss(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-04 15:37:44,365 - INFO - [load data] done in 1 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"load data\"):\n",
    "    train = feather.read_dataframe(FOLDS_PATH)\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    if LOG:\n",
    "        y = np.log1p(train['賃料'].copy())\n",
    "    else:\n",
    "        y = train['賃料'].copy()\n",
    "    folds = train[[\"fold_id\"]].copy()\n",
    "    n_train = len(train)\n",
    "    train = train.append(test).reset_index(drop=True)\n",
    "    del test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-04 15:37:45,778 - INFO - [load featuers] done in 1 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"load featuers\"):\n",
    "    #train = merge_meta(train, META_PATH, META_PATH2)\n",
    "    for f in FEATURES:\n",
    "        train = load_feature_feather(train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop done\n",
      "Memory usage of dataframe is 111.89 MB\n",
      "column =  466\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "Memory usage after optimization is: 40.73 MB\n",
      "Decreased by 63.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-04 15:37:51,980 - INFO - [preprocessing] done in 6 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"preprocessing\"):\n",
    "    for c in categorical_features:\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].astype(\"str\").values))\n",
    "        train[c] = lbl.transform(list(train[c].astype(\"str\").values))\n",
    "        \n",
    "    \"\"\"importtanceで区切るなら\"\"\"\n",
    "    if len(use_cols)==0:\n",
    "        test = train[n_train:]\n",
    "        train = train[:n_train]\n",
    "    \n",
    "    elif len(use_cols)>0:\n",
    "        test = test[use_cols]\n",
    "        train = train[use_cols]\n",
    "    \n",
    "    try:\n",
    "        train.drop(drop_columns, axis=1, inplace=True)\n",
    "        test.drop(drop_columns, axis=1, inplace=True)\n",
    "        print(\"drop done\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    train = reduce_mem_usage(train)\n",
    "    train.to_feather(f\"../features/{EXP_ID}_df.feather\")\n",
    "    \n",
    "        \n",
    "    features = list(train.columns.values)\n",
    "    \n",
    "    #gc.collect()\n",
    "    \n",
    "    #categorical_features = [cat_col for cat_col in categorical_features if cat_col not in TE_columns]\n",
    "    \n",
    "    target_test = pd.read_csv(f'../code/feature_csv/target_encoding_test.csv')\n",
    "    test[TE_columns] = target_test[TE_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3078]\tvalid's rmse: 17373.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-04 15:38:15,265 - INFO - Best Iteration: 3078\n",
      "2019-11-04 15:38:18,283 - INFO - [fold 0] done in 26 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2736]\tvalid's rmse: 20792.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-04 15:38:41,651 - INFO - Best Iteration: 2736\n",
      "2019-11-04 15:38:44,453 - INFO - [fold 1] done in 26 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n"
     ]
    }
   ],
   "source": [
    "with timer(\"train\"):\n",
    "    y_oof = np.empty(len(train), )\n",
    "    y_test = []\n",
    "    feature_importances = pd.DataFrame()\n",
    "\n",
    "    for fold_id in range(N_SPLITS):\n",
    "        with timer(f\"fold {fold_id}\"):\n",
    "            x_train, y_train = train[folds.fold_id != fold_id], y[folds.fold_id != fold_id]\n",
    "            x_val, y_val = train[folds.fold_id == fold_id], y[folds.fold_id == fold_id]\n",
    "            \n",
    "            #TE_feature = pd.read_csv(f'../code/feature_csv/target_encoding_groupfold{fold_id}.csv')\n",
    "            #x_train[TE_columns] = TE_feature[TE_feature[\"fold_id\"]!=fold_id][TE_columns]\n",
    "            #x_val[TE_columns] = TE_feature[TE_feature[\"fold_id\"]==fold_id][TE_columns]\n",
    "\n",
    "            y_pred_valid, y_pred_test, train_loss, valid_loss, importances, best_iter = train_lgbm(\n",
    "                x_train, y_train, x_val, y_val, test,\n",
    "                categorical_features=categorical_features,\n",
    "                feature_name=features,\n",
    "                fold_id=fold_id,\n",
    "                lgb_params=LGBM_PARAMS,\n",
    "                fit_params=LGBM_FIT_PARAMS,\n",
    "                model_name=EXP_ID,\n",
    "                loss_func=calc_loss,\n",
    "                rank=False,\n",
    "                calc_importances=True\n",
    "            )\n",
    "            y_oof[folds.fold_id == fold_id] = y_pred_valid\n",
    "            y_test.append(y_pred_test)\n",
    "            feature_importances = pd.concat([feature_importances, importances], axis=0, sort=False)\n",
    "\n",
    "    feature_importances.to_csv(os.path.join(SAVE_PATH, \"feature_importances.csv\"), index=False)\n",
    "    if LOG:\n",
    "        y = np.expm1(y)\n",
    "        y_oof = np.expm1(y_oof)\n",
    "        y_test = np.expm1(y_test)\n",
    "    score = calc_loss(y, y_oof)\n",
    "    y_test = np.mean(y_test, axis=0)\n",
    "    #np.save(os.path.join(SAVE_PATH, \"oof.npy\"), y_oof)\n",
    "    #np.save(os.path.join(SAVE_PATH, \"y.npy\"), y)\n",
    "    yoof_and_target = pd.DataFrame()\n",
    "    yoof_and_target[\"賃料\"] = y\n",
    "    yoof_and_target[f'{EXP_ID}'] = y_oof\n",
    "    yoof_and_target.to_csv(os.path.join(SAVE_PATH, \"yoof_and_target.csv\"), index=False)\n",
    "    yoof_and_target.to_csv(f'../output/vs_trains/{EXP_ID}.csv', index=False)\n",
    "    \n",
    "    test_result = pd.DataFrame()\n",
    "    test_result[f\"{EXP_ID}\"] = y_test\n",
    "    test_result.to_csv(f'../output/test_results/{EXP_ID}.csv', index=False)\n",
    "    \n",
    "    LOGGER.info(f'CV={score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"if fold_id does not drop \"\"\"\n",
    "#(x_train[x_train[\"fold_id\"]!=0][\"fold_id\"] == TE_feature[TE_feature[\"fold_id\"]!=0][\"fold_id\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TE_feature[TE_columns][\"所在_区\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"sub\"):\n",
    "    sub = pd.read_csv(SUB_PATH, header=None)\n",
    "    sub[1] = y_test\n",
    "    LOGGER.info(f'len sub={len(sub)}')\n",
    "    sub.to_csv(os.path.join(SAVE_PATH,f'{EXP_ID}.csv'), index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"importtances visualization\"\"\"\n",
    "cols = importances[[\"feature\", \"gain\"]].groupby(\"feature\").mean().sort_values(\n",
    "    by=\"gain\", ascending=False)[:1000].index\n",
    "\n",
    "best_features = importances.loc[importances.feature.isin(cols)]\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.barplot(x=\"gain\", y=\"feature\", data=best_features.sort_values(by=\"gain\", ascending=False))\n",
    "plt.title(f'LightGBM Features (avg over folds)_{EXP_ID}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_PATH,(f'lgbm_importances_{EXP_ID}.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features\n",
    "#TE_feature[TE_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "importances.sort_values(\"gain\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = list(importances[importances[\"gain\"]>10000][\"feature\"].values)\n",
    "len(use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoof_and_target[yoof_and_target[\"賃料\"] == yoof_and_target[\"賃料\"].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yoof_and_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
